{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to do our imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('./train.csv')\n",
    "data = np.array(training_data)\n",
    "m, n = data.shape\n",
    "\n",
    "# Shuffle the data to ensure a good mix \n",
    "np.random.shuffle(data) \n",
    "\n",
    "# Split the data into development (validation) and training sets\n",
    "data_dev = data[0:1000].T  # Test set\n",
    "Y_dev = data_dev[0]        \n",
    "X_dev = data_dev[1:]       \n",
    "X_dev = X_dev / 255.       \n",
    "\n",
    "data_train = data[1000:].T  #Training set \n",
    "Y_train = data_train[0]     \n",
    "X_train = data_train[1:]    \n",
    "X_train = X_train / 255. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 37800 training images, per image we have 28*28=784 pixels, each pixel has a number from 0 to 255 representing it's color on the greyscale.\n",
    "Now we need to make a function that initializes the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters():\n",
    "    #First layer's parameters\n",
    "    #This makes an array of 10 arrays, each array has 784 elements, that are random between -0.5 and 0.5\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "\n",
    "    # #Second layer's parameters\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1,b1,W2,b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start with forward propagation:\n",
    "But before, we need to define an activation function this can be Sigmoid, ReLU or whatever you choose, for this example, we can choose ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(array):\n",
    "    #This does it element wise, which means for every element if it's positive it will return the element\n",
    "    #If negative, it returns 0.\n",
    "    return np.maximum(0,array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a softmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(array):\n",
    "    #This is just a mathematical formula, it adds up all the sums, then say the sum of the array is \n",
    "    #20, and the value for three is 7, then three will have 7/20, which is the probability that this number \n",
    "    #is indeed a 3\n",
    "    return np.exp(array)/sum(np.exp(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(W1,b1,W2,B2,input):\n",
    "    #First layer here refers to the first hidden layer, because the input layer doesn't count\n",
    "    Z1= W1.dot(input)+b1\n",
    "    A1=ReLU(Z1)\n",
    "    #Now for the second layer, we want to get a probability, i.e a number between 0 and 1, which means\n",
    "    #we need to do a softmax function\n",
    "    Z2=W2.dot(A1)+B2\n",
    "    A2=softmax(Z2)\n",
    "    return Z1,A1,Z2,A2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for back propagation, which is basically the step that uses gradient calculus, to find the lowest error, which will be the minima of the cost function, every time we run a training example, we do forward propagation, it will give an assumption that is usually not great at the beggining, then we will tell the computer just how off it is from the actual values that it should have in the last layer, the last layer will tell the layer before it how the weights and biases should be changed, and that layer's values will tell the layer before it how the weights and biases should be changed and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we basically give this function an array of numbers, for each number, it will give \n",
    "#an array of 10 elements, where only the index of that number is 1, and the rest is zero\n",
    "#we do this so we can subtract the predicted value from the true value, and for each example\n",
    "#get an array of the difference. \n",
    "def one_hot(Y,num_classes=None):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    #To keep track the difference will look something like this:\n",
    "    #difference = [0.3-0, 0.03-0, 0.8-1, ....]\n",
    "\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T) #Adding the transpose bascially means division\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    \n",
    "    #Repeat for first layer:\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    #So these derivates now show us the gradient step, which is in the steepest ascent direction\n",
    "    #What we want to do is go in the opposite direction (in the negative direction)\n",
    "    #So we can find the steepest descent\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "#Putting everything together\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    #Initialize the Parameters\n",
    "    W1, b1, W2, b2 = init_parameters()\n",
    "\n",
    "    #Now we will start the learning loop:\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 35 == 0:\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 5 6 ... 6 9 6] [7 0 0 ... 7 9 2]\n",
      "0.09760975609756098\n",
      "[3 0 0 ... 9 9 2] [7 0 0 ... 7 9 2]\n",
      "0.3903170731707317\n",
      "[3 0 0 ... 9 9 2] [7 0 0 ... 7 9 2]\n",
      "0.514609756097561\n",
      "[3 0 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.630390243902439\n",
      "[3 5 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.7007073170731707\n",
      "[3 5 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.7432682926829268\n",
      "[3 5 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.7721463414634147\n",
      "[3 5 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.7940975609756098\n",
      "[3 5 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.8101463414634147\n",
      "[2 5 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.820780487804878\n",
      "[2 5 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.8290975609756097\n",
      "[2 5 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.8354390243902439\n",
      "[2 5 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.8414390243902439\n",
      "[2 5 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.8465853658536585\n",
      "[2 5 0 ... 7 9 2] [7 0 0 ... 7 9 2]\n",
      "0.850780487804878\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.1, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get up to 85% accuracy on the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
